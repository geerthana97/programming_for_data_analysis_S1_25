{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ§­ Task 1 â€“ Fundamental Data Understanding (Indian Air Pollution Data)**\n",
        "\n",
        "### **ðŸŽ¯ Objective :**\n",
        "\n",
        "The purpose of this task is to help you explore and understand the structure and quality of the Indian Air Pollution Dataset provided for your assessment.\n",
        "You will use Pandas to:\n",
        "\n",
        "* Combine multiple CSV files into a single dataset, and\n",
        "\n",
        "* Perform fundamental data understanding (data inspection, summary statistics, and missing value analysis)."
      ],
      "metadata": {
        "id": "U1fZAwbD5_Ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mounting the drive**"
      ],
      "metadata": {
        "id": "W8e7zPQXybKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zHklVddyEJ4U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24258871-97b6-42e2-8194-0498237f82ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd '/content/drive/MyDrive/Semester1_25/Beijing air quality'\n",
        "# please change the path according to the location of your data"
      ],
      "metadata": {
        "id": "26-GHQPZDtH4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea66f6dd-7df1-4489-ab09-5e2a0ef5fa34"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Semester1_25/Beijing air quality\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls # it shows all the content of your folder if done properly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR8z-HPzuLNA",
        "outputId": "61dc4c66-84d3-476d-9a42-2917c67aa461"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EDA__Beijing_data.ipynb\n",
            "PRSA_Data_Aotizhongxin_20130301-20170228.csv\n",
            "PRSA_Data_Changping_20130301-20170228.csv\n",
            "PRSA_Data_Dingling_20130301-20170228.csv\n",
            "PRSA_Data_Dongsi_20130301-20170228.csv\n",
            "PRSA_Data_Guanyuan_20130301-20170228.csv\n",
            "PRSA_Data_Gucheng_20130301-20170228.csv\n",
            "PRSA_Data_Huairou_20130301-20170228.csv\n",
            "PRSA_Data_Nongzhanguan_20130301-20170228.csv\n",
            "PRSA_Data_Shunyi_20130301-20170228.csv\n",
            "PRSA_Data_Tiantan_20130301-20170228.csv\n",
            "PRSA_Data_Wanliu_20130301-20170228.csv\n",
            "PRSA_Data_Wanshouxigong_20130301-20170228.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ§© What You Need to Do:**\n",
        "\n",
        "**Step 1: Import Libraries :**\n",
        "\n",
        "Start by importing the necessary Python libraries:"
      ],
      "metadata": {
        "id": "VEo6khYa6c_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "otLvwQZ35-F8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ðŸ§© Merging of the csv files:**\n",
        "\n",
        "\n",
        "\n",
        "### **Combining all the csv files in the drive path: You can use either of the merging options:**  \n",
        "---"
      ],
      "metadata": {
        "id": "0zIh_JTp7a8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ§© OPTION 1 :**"
      ],
      "metadata": {
        "id": "TendfcOi72Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd  # pandas is used for working with data tables\n",
        "import glob         # glob is used to find files by name patterns\n",
        "\n",
        "# STEP 1: The pattern \"*_data.csv\" means \"find all files that end with '_data.csv'\"\n",
        "# This will find files like: Ahmedabad_data.csv, Delhi_data.csv, Mumbai_data.csv, etc.\n",
        "city_files = glob.glob(\"*_data.csv\")\n",
        "\n",
        "# STEP 2: Create an empty list to store all our city data\n",
        "# We'll put each city's data in this list before combining them\n",
        "all_cities_data = []\n",
        "\n",
        "# STEP 3: Read each city file one by one\n",
        "for file_name in city_files:\n",
        "    # Read the current city's CSV file into a DataFrame\n",
        "    # A DataFrame is like a spreadsheet table in Python\n",
        "    city_df = pd.read_csv(file_name)\n",
        "\n",
        "    # Add this city's data to our list\n",
        "    all_cities_data.append(city_df)\n",
        "\n",
        "    # Optional: Print which file we just read\n",
        "    print(f\"Loaded: {file_name}\")\n",
        "\n",
        "# STEP 4: Combine all city data into one big table\n",
        "# pd.concat() joins all the DataFrames in our list together\n",
        "# ignore_index=True makes sure the row numbers are continuous (0, 1, 2, 3...)\n",
        "combined_data = pd.concat(all_cities_data, ignore_index=True)\n",
        "\n",
        "# STEP 5: Save the combined data to a new CSV file\n",
        "# index=False means don't save the row numbers as a separate column\n",
        "combined_data.to_csv(\"all_cities_combined.csv\", index=False)\n",
        "\n",
        "# STEP 6: Show us what we accomplished\n",
        "# len(city_files) = count of how many city files we combined\n",
        "# len(combined_data) = total number of rows in the final combined file\n",
        "print(f\"SUCCESS: Combined {len(city_files)} city files into one file with {len(combined_data)} total rows\")\n",
        "print(\"The combined file is saved as: all_cities_combined.csv\")"
      ],
      "metadata": {
        "id": "RuAcWEZI3eCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv('all_cities_combined.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "JrvDP3Jq-x50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ðŸ§© OPTION 2 :**"
      ],
      "metadata": {
        "id": "fRuTIqcR7-qK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The process involves iterating through all files in a specified directory (drive_path) to identify those with a .csv extension. Each identified CSV file is then read into a Pandas DataFrame, which is subsequently appended to a list (dataframes). Once all the CSV files are processed, the individual DataFrames in the list are combined into a single consolidated DataFrame (all_data) using the pd.concat() function. This consolidation ensures that the combined data is reindexed, creating a unified dataset for further analysis."
      ],
      "metadata": {
        "id": "apBTIrvohmPx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfT7CRmhNA3s"
      },
      "outputs": [],
      "source": [
        "drive_path = '/content/drive/MyDrive/Semester1_25/Beijing air quality'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wEToJqp4UPt"
      },
      "outputs": [],
      "source": [
        "dataframes = []\n",
        "for filename in os.listdir(drive_path):\n",
        "    if filename.endswith('.csv'):  # Check if the file is a CSV file\n",
        "        file_path = os.path.join(drive_path, filename)\n",
        "        df = pd.read_csv(file_path)  # Read the CSV file into a DataFrame\n",
        "        dataframes.append(df)  # Add the DataFrame to the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM6z0uWnN1Qn"
      },
      "outputs": [],
      "source": [
        "df1 = pd.concat(dataframes, ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ§© Perform Fundamental Data Understanding :**\n",
        "\n",
        "**Once you have the merged dataset, explore and understand its structure.**"
      ],
      "metadata": {
        "id": "qpj4CmVl86Zd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTA0B5rajLMW"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULqD4b8KjQKv"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pSlzvmpjVF6",
        "outputId": "0810c2c6-f3a7-42fb-c507-9f656ffeebad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of Rows: 420768, No of Columns: 18\n"
          ]
        }
      ],
      "source": [
        "df.shape\n",
        "print(f'No of Rows: {df1.shape[0]}, No of Columns: {df1.shape[1]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDGbj0xxA0N7"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "BfWPrGfvDxLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "8EB1mnxwEKHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTklEYlTq-MG"
      },
      "source": [
        "### **Total number of stations in the dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYXh9uLOrB92"
      },
      "outputs": [],
      "source": [
        "stations = df1['station'].value_counts()\n",
        "print(f'Total number of stations in the dataset : {len(stations)}')\n",
        "stations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43UvgmyaizgR"
      },
      "source": [
        "### **Displaying the percentage of missing value**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NXsXA19iyuh"
      },
      "outputs": [],
      "source": [
        "def missing_values_table(df1):\n",
        "    # Total missing values\n",
        "    mis_val = df1.isnull().sum()\n",
        "\n",
        "    # Percentage of missing values\n",
        "    mis_val_percent = 100 * mis_val / len(df1)\n",
        "\n",
        "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
        "\n",
        "    mis_val_table = mis_val_table.rename(columns={0: 'Missing Values', 1: '% of Total Values'})\n",
        "\n",
        "    # Sort the table by percentage of missing descending\n",
        "    mis_val_table = mis_val_table.sort_values('% of Total Values', ascending=False)\n",
        "\n",
        "    return mis_val_table\n",
        "\n",
        "missing_values = missing_values_table(df1)\n",
        "display(missing_values.style.background_gradient(cmap='Blues'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ“Š Analysis Questions to Answer :**\n",
        "\n",
        "**Include short written answers (3â€“5 sentences) to these:**\n",
        "\n",
        "* How many rows and columns are in your merged dataset?\n",
        "\n",
        "* Which pollutants are included, and which have the most missing data?\n",
        "\n",
        "* How many unique cities or stations are there?\n",
        "\n",
        "* What are the average levels of key pollutants?\n",
        "\n",
        "* Are there any immediate data quality issues (e.g., missing or inconsistent values)?"
      ],
      "metadata": {
        "id": "ektSrqq6Buyj"
      }
    }
  ]
}